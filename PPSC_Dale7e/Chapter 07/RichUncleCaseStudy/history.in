The Abacus (which appeared in the sixteenth century) was the
first calculator.  In the middle of the seventeenth century
Blaise Pascal, a French mathematician, built and sold gear-
driven mechanical machines which performed whole number
addition and subtraction.  (Yes, the language Pascal is named
for him.)

Later in the seventeenth century a German mathematician
Gottfried Wilhelm von Leibniz built the first mechanical
device designed to do all four whole number operations:
addition, subtraction, multiplication and division.  The
state of mechanical gears and levers at that time was such
that the Leibniz machine was not very reliable.

It wasn't until the nineteenth century that the next major
step was taken, this time by a British mathematician.
Charles Babbage designed what he called his analytical
engine.  His design was too complex for him to build with the
technology of his day, so his design was never implemented.
His vision, however, included the important components of
today's computers.  His design was the first to include a
memory so that intermediate values did not have to be re-
entered.  His design also included the input of both numbers
and mechanical steps on a type of punched card developed in
the late eighteenth century by Joseph Jacquard to be used
with a mechanical loom.

Ada Agusta, Countess of Lovelace, was a most romantic figure
in the history of computing.  Ada, the daughter of Lord Byron
(the English poet), was herself quite a mathematician.  She
became interested in Babbage's work on the analytical engine
and extended his ideas (and corrected some of his errors).
Ada is credited with being the first programmer.  The concept
of the loop--a series of instructions that repeat--is
attributed to her.  Ada, the language used by the Department
of Defense, is named for her.

During the later part of the nineteenth century and the
beginning of the twentieth century advances were made
rapidly.  William Seward Burroughs produced and sold a
mechanical adding machine (that worked!).  Dr. Herman
Hollerith developed the first electro-mechanical tabulator
which used a punched card.  Data was represented by holes
punched on the card in certain positions.   Descendants of
this card are still in use today.  Dr. Hollerith formed a
company that is known today as IBM.

In 1936 a theoretical development took place that had nothing
to do with hardware per se but profoundly influenced the
field of Computer Science.  Alan M. Turing, another British
mathematician, invented an abstract mathematical model called
a Turing machine, laying the foundation for a major area of
computing theory..  The most prestigious award given in
Computer Science (equivalent to the Fielding Medal in
Mathematics or a Nobel Prize in other sciences) is the Turing
Award, named for Alan Turing.  A current Broadway play deals
with his life.  Analysis of the capabilities of Turing
machines is a part of the theoretical studies of all Computer
Science students.

By the outbreak of World War II, several computers were under
design and construction.   The Harvard Mark I and the ENIAC
are two of the more famous machines of the era.  John von
Neumann (an American mathematician this time) who had been a
consultant on the ENIAC project started work on another
machine known as EDVAC which was completed in 1950.  In 1951
the first commercial computer, UNIVAC I, was delivered to the
Bureau of the Census.

Von Neumann is credited with the development of two very
important concepts.  The first is that the instructions that
operate the machine should be stored in the machine along
with the data to be operated on.  The second is that data
(and instructions) should be represented in the binary number
system rather than the decimal number system.


The general organization of a computer has remained constant
over the last 40 years.  This organization or architecture is based
on the ideas of von Neumann and bears his name.  The
essence of this architecture is sequentiality.  That is,
instructions (a program) are stored in sequential memory
locations in binary form and are executed in sequence, one
after the other.
